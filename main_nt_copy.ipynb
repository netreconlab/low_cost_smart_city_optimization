{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import peartree as pt\n",
    "import random\n",
    "\n",
    "############### MY MODULES ###############\n",
    "import sim\n",
    "from sensor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ GLOBAL VARIABLES #############\n",
    "\n",
    "def reset_network():\n",
    "    global_variables = {\n",
    "        'time_table': None,\n",
    "        'feed': None,\n",
    "        'G': None,\n",
    "        'stop_times': None,\n",
    "        'routes': None,\n",
    "        'trips': None,\n",
    "        'all_routes': None,\n",
    "        'all_trips': None,\n",
    "\n",
    "        'stop_times_dict': None,\n",
    "        'trips_per_stop': None,\n",
    "        'routes_per_stop': None,\n",
    "        'stop_ranks': None,\n",
    "        'route_subgraphs': None,\n",
    "        'edge_departures': None,\n",
    "        'trip_subgraphs': None,\n",
    "        'stops_per_trip': None\n",
    "    }\n",
    "    \n",
    "    globals().update(global_variables)\n",
    "    \n",
    "def reset_sim():\n",
    "    global_variables = {\n",
    "        'error': 0,\n",
    "        'routes_per_gateway': None,\n",
    "        'gateways_per_route': None,\n",
    "        'all_gateways': None,\n",
    "        'all_sensors': None,\n",
    "        'sensor_count': None,\n",
    "        'sensor_objects': None,\n",
    "    }\n",
    "\n",
    "    globals().update(global_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## HELPER FUNCTIONS ##################\n",
    "\n",
    "def get_stopid(node_name):\n",
    "    return node_name.split('_')[-1]\n",
    "\n",
    "\n",
    "def namify_stop(g_name,stop_id):\n",
    "    return \"{0}_{1}\".format(g_name,stop_id)\n",
    "\n",
    "\n",
    "def invert_dict(d):\n",
    "    inverted_d = defaultdict(set)\n",
    "    for k in d.keys():\n",
    "        for v in d[k]:\n",
    "            inverted_d[v].add(k)\n",
    "    return inverted_d\n",
    "\n",
    "# I don't think this is useful\n",
    "def get_routes_per_stop_id(stop_id):\n",
    "    for stop_id in time_table.stop_id.unique():\n",
    "            routes = time_table[time_table.stop_id == stop_id].route_id.unique()\n",
    "            return set(routes)\n",
    "\n",
    "\n",
    "def get_time_to_next_departure(current_time, departure_list):\n",
    "    try:\n",
    "        next_departure = min(v for v in departure_list if v >= current_time)\n",
    "        wait_time = next_departure - current_time\n",
    "    except:\n",
    "        wait_time = None\n",
    "\n",
    "    return wait_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network():\n",
    "    global feed,G\n",
    "\n",
    "    feed = pt.get_representative_feed('data/gtfs/' + sim.network_file)\n",
    "    G = pt.load_feed_as_graph(feed, sim.start, sim.end, interpolate_times=True)\n",
    "    \n",
    "def load_stop_times():\n",
    "    global stop_times, routes, trips, time_table\n",
    "\n",
    "\n",
    "    stop_times = feed.stop_times\n",
    "    routes = feed.routes\n",
    "    trips = feed.trips\n",
    "\n",
    "    stoptimes_trips = stop_times.merge(trips, left_on='trip_id', right_on='trip_id')\n",
    "    stoptimes_trips_routes = stoptimes_trips.merge(routes, left_on='route_id', right_on='route_id')\n",
    "\n",
    "    columns = ['route_id',\n",
    "               'service_id',\n",
    "               'trip_id',\n",
    "               #'trip_headsign',\n",
    "               'direction_id',\n",
    "               #'block_id',\n",
    "               #'shape_id',\n",
    "               #'route_short_name',\n",
    "               #'route_long_name',\n",
    "               'route_type',\n",
    "               'arrival_time',\n",
    "               'departure_time',\n",
    "               'stop_id',\n",
    "               'stop_sequence'\n",
    "              ]\n",
    "\n",
    "    time_table = stoptimes_trips_routes[columns]\n",
    "\n",
    "def format_stop_times():\n",
    "    global time_table, all_trips, all_routes\n",
    "\n",
    "    #time_table = pt.summarizer._trim_stop_times_by_timeframe(time_table, sim.start, sim.end)\n",
    "\n",
    "    time_table = time_table[~time_table['route_id'].isnull()]\n",
    "\n",
    "    time_table = pt.summarizer._linearly_interpolate_infill_times(\n",
    "        time_table,\n",
    "        use_multiprocessing=True)\n",
    "\n",
    "    if 'direction_id' in time_table:\n",
    "        # If there is such column then check if it contains NaN\n",
    "        has_nan = time_table['direction_id'].isnull()\n",
    "        if sum(has_nan) > 0:\n",
    "            # If it has no full coverage in direction_id, drop the column\n",
    "            time_table.drop('direction_id', axis=1, inplace=True)\n",
    "\n",
    "    # all_routes = set(feed.routes.route_id.values)\n",
    "    all_routes = set(time_table.route_id.unique())\n",
    "    all_trips = set(time_table.trip_id.unique())\n",
    "\n",
    "\n",
    "\n",
    "def analyze_stops():\n",
    "    global stop_times_dict, trips_per_stop, routes_per_stop, stop_ranks\n",
    "    stop_times_dict = defaultdict(dict)\n",
    "    trips_per_stop = defaultdict(set)\n",
    "    routes_per_stop = defaultdict(set)\n",
    "    routes_per_stop = defaultdict(set)\n",
    "    stop_ranks = OrderedDict()\n",
    "\n",
    "    for i,row in time_table.iterrows():\n",
    "       trips_per_stop[row.stop_id].add(row.trip_id)\n",
    "       routes_per_stop[row.stop_id].add(row.route_id)\n",
    "\n",
    "    d = {}\n",
    "    for k,v in routes_per_stop.items():\n",
    "        d[k] = len(v)\n",
    "\n",
    "    for k in sorted(d, key=d.get, reverse=True):\n",
    "        stop_ranks[k] = d[k]\n",
    "    #stop_ranks = {k:d[k] for k in sorted(d, key=d.get, reverse=True)} \n",
    "    \n",
    "def assign_gateways_to_nodes():\n",
    "    global all_gateways #input\n",
    "    global G #output\n",
    "\n",
    "    attr = {gw:True for gw in all_gateways}\n",
    "    nx.set_node_attributes(G, name='is_gateway', values=attr)\n",
    "\n",
    "    return G\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     1,
     60
    ]
   },
   "outputs": [],
   "source": [
    "#### Add departure times of source node to edges\n",
    "def get_departure_times_per_edge_per_route():\n",
    "    import pandas as pd\n",
    "\n",
    "    global time_table  # input\n",
    "    global edge_departures  # output\n",
    "\n",
    "    has_dir_col = 'direction_id' in time_table.columns.values\n",
    "\n",
    "    all_deps = []\n",
    "    all_route_ids = []\n",
    "    all_trip_ids = []\n",
    "    all_from_stop_ids = []\n",
    "    all_to_stop_ids = []\n",
    "\n",
    "    for trip_id in time_table.trip_id.unique():\n",
    "\n",
    "        tst_sub = time_table[time_table.trip_id == trip_id]\n",
    "        route = tst_sub.route_id.values[0]\n",
    "\n",
    "        # Just in case both directions are under the same trip id\n",
    "        for direction in [0, 1]:\n",
    "            # Support situations where direction_id is absent from the\n",
    "            # GTFS data. In such situations, include all trip and stop\n",
    "            # time data, instead of trying to split on that column\n",
    "            # (since it would not exist).\n",
    "            if has_dir_col:\n",
    "                dir_mask = (tst_sub.direction_id == direction)\n",
    "                tst_sub_dir = tst_sub[dir_mask]\n",
    "            else:\n",
    "                tst_sub_dir = tst_sub.copy()\n",
    "\n",
    "            tst_sub_dir = tst_sub_dir.sort_values('stop_sequence')\n",
    "            \n",
    "            deps = tst_sub_dir.departure_time[:-1]\n",
    "\n",
    "            # Add each resulting list to the running array totals\n",
    "            all_deps += list(deps)\n",
    "\n",
    "            from_ids = tst_sub_dir.stop_id[:-1].values\n",
    "            all_from_stop_ids += list(from_ids)\n",
    "\n",
    "            to_ids = tst_sub_dir.stop_id[1:].values\n",
    "            all_to_stop_ids += list(to_ids)\n",
    "\n",
    "            all_route_ids.extend([route] * len(deps))\n",
    "            all_trip_ids.extend([trip_id] * len(deps))\n",
    "\n",
    "    # Only return a dataframe if there is contents to populate\n",
    "    # it with\n",
    "    if len(all_deps) > 0:\n",
    "        # Now place results in data frame\n",
    "        edge_departures = pd.DataFrame({\n",
    "            'from_stop_id': all_from_stop_ids,\n",
    "            'to_stop_id': all_to_stop_ids,\n",
    "            'departure_times': all_deps,\n",
    "            'route_id': all_route_ids,\n",
    "            'trip_id': all_trip_ids})\n",
    "\n",
    "        \n",
    "def add_departure_to_edge():\n",
    "    global edge_departures  # input\n",
    "    global G  # output\n",
    "\n",
    "    for i, row in edge_departures.drop_duplicates(['from_stop_id', 'to_stop_id']).iterrows():\n",
    "        u,v = row.from_stop_id, row.to_stop_id\n",
    "\n",
    "        dep_mask = (edge_departures['from_stop_id'] == u) & (edge_departures['to_stop_id'] == v)\n",
    "        #dep_list = edge_deps[dep_mask].deps.values\n",
    "        dep_list = edge_departures[dep_mask][['route_id', 'departure_times']].sort_values(['departure_times'])\n",
    "\n",
    "        dep_per_route = dep_list.groupby('route_id')['departure_times'].apply(lambda x: x.tolist()).to_dict(into=OrderedDict)\n",
    "\n",
    "        u,v =  namify_stop(G.name,u), namify_stop(G.name,v)\n",
    "\n",
    "        #TODO:: find out why you have to do this\n",
    "        if u in G and v in G[u]:\n",
    "            G[u][v][0]['departure_time'] = dep_per_route\n",
    "\n",
    "\n",
    "    #test to make sure all edges is serviced\n",
    "    for x in G.edges(keys=True,data=True):\n",
    "        if 'departure_time' not in x[3]:\n",
    "            print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = add_departure_to_edge()\n",
    "#g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Randomly selects stops to serve as sensors\n",
    "def randomly_select_sensor_locations():\n",
    "    global G # input\n",
    "    global all_sensors, sensor_count # output\n",
    "\n",
    "\n",
    "    all_stops = set(G.nodes)\n",
    "    sensor_count = round(len(all_stops) * sim.pct_stops_as_sensors / 100)\n",
    "\n",
    "    eligible_stops = list(all_stops - set(all_gateways)) #remove gateways from the list\n",
    "    all_sensors = np.random.choice(eligible_stops, size=sensor_count, replace=False)\n",
    "\n",
    "\n",
    "## Mark selected nodes as sensors\n",
    "def assign_sensors_to_nodes():\n",
    "    global all_sensors  # input\n",
    "    global G  # output\n",
    "\n",
    "    attr = {sensor:True for sensor in all_sensors}\n",
    "\n",
    "    nx.set_node_attributes(G, name='is_sensor', values=attr)\n",
    "\n",
    "\n",
    "def generate_sensors():\n",
    "    global all_sensors, routes_per_stop # input\n",
    "    global sensor_objects # output\n",
    "\n",
    "    sensor_objects = {}\n",
    "\n",
    "    msg_gen_rate = np.random.randint(low = sim.msg_gen_rate_range[0], high= sim.msg_gen_rate_range[1], size=len(all_sensors)) # 10mins to 12 hours\n",
    "    start_time = np.random.randint(low = sim.msg_gen_rate_range[0], high=sim.msg_gen_rate_range[1], size=len(all_sensors)) # 0 to 1 hour\n",
    "    np.random.shuffle(start_time)\n",
    "\n",
    "    print(sum(msg_gen_rate), sum(start_time))\n",
    "\n",
    "    #exit()\n",
    "\n",
    "\n",
    "    for i,sensor_name in enumerate(all_sensors):\n",
    "        #print(i,sensor_name)\n",
    "        #r = get_routes_per_stop_id(get_stopid(sensor_name))\n",
    "        r = routes_per_stop[get_stopid(sensor_name)]\n",
    "\n",
    "        s = OnRouteSensor(name=sensor_name, routes=r, start_time=start_time[i], msg_gen_rate=msg_gen_rate[i], msg_ttl=None, data_size=None)\n",
    "        sensor_objects[sensor_name]=s\n",
    "\n",
    "\n",
    "def generate_route_subgraphs():\n",
    "    global G, routes_per_stop, all_routes # input\n",
    "    global route_subgraphs, stops_per_route # output\n",
    "\n",
    "    route_subgraphs = {}\n",
    "    stops_per_route = invert_dict(routes_per_stop)\n",
    "\n",
    "    for r in all_routes:\n",
    "        sub_nodes = [namify_stop(G.name, s) for s in stops_per_route[r]]\n",
    "        # G.remove_nodes_from([n for n in G if n not in set(nodes)])\n",
    "        sub_graph = G.subgraph(sub_nodes).copy()\n",
    "        route_subgraphs[r] = sub_graph\n",
    "\n",
    "\n",
    "\n",
    "def calculate_delay(routes, sensor, time):\n",
    "    \"\"\"\n",
    "    find shortest path from sensor node to a gateway node in the graph, weight is edge cost,\n",
    "    while factoring in duration from current time to next next dept time for that edge.\n",
    "\n",
    "    save gen_time and latency to sensor object\n",
    "\n",
    "    remember departure time, distance is in seconds\n",
    "    while \"time\", gen_time,start_time is in minutes.\n",
    "    so remember to convert it.\n",
    "    \"\"\"\n",
    "    global G, route_subgraphs, gateways_per_route  # inputs\n",
    "\n",
    "    global error\n",
    "\n",
    "    import sys\n",
    "    waiting_time = None\n",
    "    shortest_distance, shortest_path = sys.float_info.max, None  # to any gateway\n",
    "\n",
    "\n",
    "    for r in routes:\n",
    "        for gateway in gateways_per_route[r]:\n",
    "\n",
    "            g = route_subgraphs[r].copy()\n",
    "\n",
    "            wait_time = None\n",
    "\n",
    "            try:\n",
    "                distance, path = nx.single_source_dijkstra(g, sensor.name, namify_stop(G.name, gateway), weight='length')\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "            while len(path) > 1:\n",
    "                '''\n",
    "                make sure then you limit duration to 24 hours. later if time is greater than 24\n",
    "                message is not delivered\n",
    "                '''\n",
    "\n",
    "                # TODO:: error rate too high.. fix it.\n",
    "                #print(path)\n",
    "                departure_list = g[sensor.name][path[1]][0]['departure_time'].get(r, None)\n",
    "\n",
    "                #print(departure_list)\n",
    "                if departure_list == None:\n",
    "                    # print(\"no departure time found\")\n",
    "                    break\n",
    "                    #g.remove_node(path[1])\n",
    "                    #continue\n",
    "\n",
    "                else:\n",
    "                    wait_time = get_time_to_next_departure(current_time=time, departure_list=departure_list)\n",
    "                    break\n",
    "\n",
    "\n",
    "            if wait_time != None:\n",
    "\n",
    "                if distance + wait_time < shortest_distance:\n",
    "                    shortest_distance, shortest_path = distance + wait_time, path\n",
    "                    waiting_time = wait_time\n",
    "                    #break\n",
    "\n",
    "    if waiting_time == None:\n",
    "        shortest_distance = None\n",
    "        error +=1\n",
    "\n",
    "\n",
    "    sensor.gen_times.append(time)  # in sec\n",
    "    sensor.msg_latencies.append(shortest_distance)  # in sec\n",
    "    sensor.waiting_time.append(waiting_time)\n",
    "    sensor.hops.append(shortest_path)\n",
    "\n",
    "\n",
    "\n",
    "def store_results():\n",
    "    import json\n",
    "    from collections import defaultdict\n",
    "    final_result = defaultdict(list)\n",
    "\n",
    "    final_result['sim_time'] = sim.duration\n",
    "\n",
    "    # print(sensor_objects.values())\n",
    "    # type(sensor_objects.values()[0])\n",
    "\n",
    "    for s in sensor_objects.values():\n",
    "\n",
    "        data = {\n",
    "            'delivery_rate': None,\n",
    "            'no_of_routes': len(s.routes),\n",
    "            'all_latencies': s.msg_latencies,\n",
    "            'all_waiting_times': s.waiting_time ,\n",
    "            'all_gen_times': s.gen_times,\n",
    "            'all_hops': s.hops,\n",
    "            'delivered_latencies': [],\n",
    "            'delivered_gen_times': [],\n",
    "            'delivered_waiting_times':[],\n",
    "            'delivered_hops':[],\n",
    "        }\n",
    "\n",
    "        for i in range(len(s.msg_latencies)):\n",
    "            if (s.msg_latencies[i] != None) and (s.gen_times[i] + s.msg_latencies[i] < sim.duration * 60):\n",
    "                data['delivered_latencies'].append(s.msg_latencies[i])\n",
    "                data['delivered_gen_times'].append(s.gen_times[i])\n",
    "\n",
    "                data['delivered_waiting_times'].append(s.waiting_time[i])\n",
    "                data['delivered_hops'].append(s.hops[i])\n",
    "\n",
    "        # print(len(s.gen_times))\n",
    "\n",
    "        if (len(s.gen_times) != 0):\n",
    "            data['delivery_rate'] = len(data['delivered_latencies']) / len(s.gen_times)\n",
    "\n",
    "        final_result['ons'].append(data)\n",
    "\n",
    "    with open('results/{0}_data_{1}.txt'.format(sim.network_file, sim.seed), 'w') as outfile:\n",
    "        json.dump(final_result, outfile, indent=True)\n",
    "\n",
    "    print(\"Results Stored!\")\n",
    "\n",
    "\n",
    "def run_simulation():\n",
    "    global sensor_objects, routes_per_stop\n",
    "    global error\n",
    "\n",
    "    for time in range(int(sim.start/60), sim.duration + 1):\n",
    "        for name, sensor in sensor_objects.items():\n",
    "            if sensor.generate_msg(time):\n",
    "                routes = routes_per_stop[get_stopid(sensor.name)]\n",
    "                # change time to secs\n",
    "                calculate_delay(routes, sensor, time * 60)\n",
    "\n",
    "    print(\"Simulation Completed! for seed_{0}\".format(sim.seed))\n",
    "    print(\"error: \" + error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_gateways1():\n",
    "    # minimal based on ranks\n",
    "    global all_routes, routes_per_stop, stop_ranks #inputs\n",
    "    global routes_per_gateway, gateways_per_route, all_gateways #outputs\n",
    "\n",
    "\n",
    "    selected_gw = []\n",
    "    routes_per_gateway = defaultdict(set)\n",
    "    unserved_routes = all_routes.copy()\n",
    "\n",
    "    for stop_id,rank in stop_ranks.items():\n",
    "\n",
    "        rs = routes_per_stop[stop_id]\n",
    "        route_to_serve = rs.intersection(unserved_routes)\n",
    "\n",
    "        if len(route_to_serve) != 0:\n",
    "\n",
    "            selected_gw.append(stop_id)\n",
    "            unserved_routes.difference_update(route_to_serve)\n",
    "\n",
    "\n",
    "            routes_per_gateway[stop_id].update(routes_per_stop[stop_id])\n",
    "\n",
    "            if len(unserved_routes) == 0:\n",
    "                break\n",
    "\n",
    "    #routes_per_gateway = select_optimal_gateways(all_routes, routes_per_stop, stop_ranks)\n",
    "    gateways_per_route = invert_dict(routes_per_gateway)\n",
    "    all_gateways = set(namify_stop(G.name, x) for x in routes_per_gateway.keys())\n",
    "    #print(all_gateways, '\\n', gateways_per_route, '\\n', routes_per_gateway, '\\n', end='\\n')\n",
    "\n",
    "def select_gateways2():\n",
    "    # minimal based on ranks\n",
    "    global all_routes, stops_per_route, routes_per_stop  # inputs\n",
    "    global all_gateways, gateways_per_route, routes_per_gateway   # outputs\n",
    "\n",
    "    all_gateways = set()\n",
    "    routes_per_gateway = defaultdict(set)\n",
    "\n",
    "    for stops in stops_per_route.values():\n",
    "        # print(stops)\n",
    "        gw = np.random.choice(list(stops), random.randint(1, 3), replace=False)\n",
    "\n",
    "        for stop_id in gw:\n",
    "            #print(routes_per_gateway, routes_per_stop)\n",
    "            routes_per_gateway[stop_id].update(routes_per_stop[stop_id])\n",
    "\n",
    "        all_gateways.update(namify_stop(G.name, gw))\n",
    "\n",
    "    gateways_per_route = invert_dict(routes_per_gateway)\n",
    "\n",
    "\n",
    "def print_stats():\n",
    "    global all_routes, all_gateways, stop_ranks\n",
    "    print(\"{} Routes, {} Gateways, {} stops\".format(len(all_routes), len(all_gateways), len(stop_ranks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trip_subgraphs():\n",
    "    global G, trips_per_stop, all_trips # input\n",
    "    global trip_subgraphs, stops_per_trip # output\n",
    "\n",
    "    trip_subgraphs = {}\n",
    "    stops_per_trip = invert_dict(trips_per_stop)\n",
    "\n",
    "    for t in all_trips:\n",
    "        sub_nodes = [namify_stop(G.name, s) for s in stops_per_trip[t]]\n",
    "        # G.remove_nodes_from([n for n in G if n not in set(nodes)])\n",
    "        sub_graph = G.subgraph(sub_nodes).copy()\n",
    "        trip_subgraphs[t] = sub_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTFS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'slice(1, None, None)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-0135384e0351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mload_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mload_stop_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mformat_stop_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-df029c0b0eee>\u001b[0m in \u001b[0;36mload_network\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_representative_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/gtfs/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_feed_as_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_stop_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/paths.py\u001b[0m in \u001b[0;36mload_feed_as_graph\u001b[0;34m(feed, start_time, end_time, name, existing_graph, connection_threshold, walk_speed_kmph, stop_cost_method, fallback_stop_cost, interpolate_times, impute_walk_transfers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    196\u001b[0m                                                            \u001b[0minterpolate_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                                                            \u001b[0mstop_cost_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                                                            use_multiprocessing)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;31m# This is a flag used to check if we need to run any additional steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/graph.py\u001b[0m in \u001b[0;36mgenerate_summary_graph_elements\u001b[0;34m(feed, target_time_start, target_time_end, fallback_stop_cost, interpolate_times, stop_cost_method, use_multiprocessing)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0minterpolate_times\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mstop_cost_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         use_multiprocessing)\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;31m# Same sanity checks on the output before we continue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/summarizer.py\u001b[0m in \u001b[0;36mgenerate_edge_and_wait_values\u001b[0;34m(feed, target_time_start, target_time_end, interpolate_times, stop_cost_method, use_multiprocessing)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mstop_cost_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         use_multiprocessing)\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_edge_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_wait_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/summarizer.py\u001b[0m in \u001b[0;36m_generate_route_processing_results\u001b[0;34m(target_route_ids, target_time_start, target_time_end, ftrips, stop_times, feed_stops, stop_cost_method, use_multiprocessing)\u001b[0m\n\u001b[1;32m    360\u001b[0m             stop_cost_method)\n\u001b[1;32m    361\u001b[0m         results = [route_analyzer.generate_route_costs(rid)\n\u001b[0;32m--> 362\u001b[0;31m                    for rid in target_route_ids]\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Route costing complete. Execution time: {}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/summarizer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    360\u001b[0m             stop_cost_method)\n\u001b[1;32m    361\u001b[0m         results = [route_analyzer.generate_route_costs(rid)\n\u001b[0;32m--> 362\u001b[0;31m                    for rid in target_route_ids]\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Route costing complete. Execution time: {}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/parallel.py\u001b[0m in \u001b[0;36mgenerate_route_costs\u001b[0;34m(self, route_id)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# Get all edge costs for this route and add to the running total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0medge_costs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_all_observed_edge_costs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrips_and_stop_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtst_sub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_costs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/peartree/parallel.py\u001b[0m in \u001b[0;36mgenerate_all_observed_edge_costs\u001b[0;34m(trips_and_stop_times)\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mtst_sub_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_sub_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stop_sequence'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_sub_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeparture_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtst_sub_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrival_time\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;31m# Use .values to strip existing indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    765\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   3116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3117\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 3118\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   3119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for network in sim.network_file_list:\n",
    "    reset_network()\n",
    "    sim.network_file = network\n",
    "    \n",
    "    load_network()\n",
    "    load_stop_times()\n",
    "    format_stop_times()\n",
    "    analyze_stops()\n",
    "    get_departure_times_per_edge_per_route()\n",
    "    add_departure_to_edge()\n",
    "    \n",
    "    generate_route_subgraphs()\n",
    "    generate_trip_subgraphs()\n",
    "    \n",
    "    for seed in range(0, sim.no_of_seeds):\n",
    "        reset_sim()\n",
    "\n",
    "        sim.seed = seed\n",
    "        \n",
    "        np.random.seed(sim.seed)\n",
    "        random.seed(sim.seed)\n",
    "\n",
    "        select_gateways1()\n",
    "        print_stats()\n",
    "        assign_gateways_to_nodes()\n",
    "\n",
    "        randomly_select_sensor_locations()\n",
    "        assign_sensors_to_nodes()\n",
    "        generate_sensors()\n",
    "        generate_route_subgraphs()\n",
    "        run_simulation()\n",
    "        store_results()\n",
    "\n",
    "        reset_sim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gateway selection\n",
    "for network in sim.network_file_list:\n",
    "    reset_network()\n",
    "    sim.network_file = network\n",
    "    \n",
    "    load_network()\n",
    "    load_stop_times()\n",
    "    format_stop_times()\n",
    "    analyze_stops()\n",
    "    get_departure_times_per_edge_per_route()\n",
    "    add_departure_to_edge()\n",
    "    \n",
    "    generate_route_subgraphs()\n",
    "    generate_trip_subgraphs()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 Routes, 7 Gateways, 571 stops\n",
      "10057 9985\n"
     ]
    }
   ],
   "source": [
    "reset_sim()\n",
    "sim.seed = 0\n",
    "np.random.seed(sim.seed)\n",
    "random.seed(sim.seed)\n",
    "\n",
    "greedy_im(graph, k, prob=1, n_iters=10)\n",
    "#select_gateways1()\n",
    "#print_stats()\n",
    "#assign_gateways_to_nodes()\n",
    "\n",
    "#randomly_select_sensor_locations()\n",
    "#assign_sensors_to_nodes()\n",
    "#generate_sensors()\n",
    "#generate_route_subgraphs()\n",
    "#run_simulation()\n",
    "#store_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_delay(graph, sensor_nodes, scenerios):\n",
    "    total_delay = 0\n",
    "\n",
    "    # simulate the spread process over multiple runs\n",
    "    for i in range(n_iters):\n",
    "        np.random.seed(i)\n",
    "        active = seed_nodes[:]\n",
    "        new_active = seed_nodes[:]\n",
    "        \n",
    "        # for each newly activated nodes, find its neighbors that becomes activated\n",
    "        while new_active:\n",
    "            activated_nodes = []\n",
    "            for node in new_active:\n",
    "                neighbors = graph.neighbors(node, mode='out')\n",
    "                success = np.random.uniform(0, 1, len(neighbors)) < prob\n",
    "                activated_nodes += list(np.extract(success, neighbors))\n",
    "\n",
    "            # ensure the newly activated nodes doesn't already exist\n",
    "            # in the final list of activated nodes before adding them\n",
    "            # to the final list\n",
    "            new_active = list(set(activated_nodes) - set(active))\n",
    "            active += new_active\n",
    "\n",
    "        total_spead += len(active)\n",
    "\n",
    "    return total_spead / len(scenerios)\n",
    "\n",
    "\n",
    "def greedy_im(graph, k, prob=1, n_iters=10):\n",
    "    \"\"\"\n",
    "    Find k nodes with the largest spread (determined by IC) from a igraph graph\n",
    "    using the Greedy Algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    # we will be storing elapsed time and spreads along the way, in a setting where\n",
    "    # we only care about the final solution, we don't need to record these\n",
    "    # additional information\n",
    "    elapsed = []\n",
    "    spreads = []\n",
    "    gateways = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for _ in range(k):\n",
    "        best_node = -1\n",
    "        best_spread = np.inf\n",
    "\n",
    "        # loop over nodes that are not yet in our final solution\n",
    "        # to find biggest marginal gain\n",
    "        nodes = set(graph.nodes()) - set(gateways)\n",
    "        for node in nodes:\n",
    "            delay = compute_delay(graph, gateways + [node], scenerios)\n",
    "            if delay < best_delay:\n",
    "                best_delay = delay\n",
    "                best_node = node\n",
    "\n",
    "        solution.append(best_node)\n",
    "        spreads.append(best_delay)\n",
    "\n",
    "        elapse = round(time.time() - start_time, 3)\n",
    "        elapsed.append(elapse)\n",
    "\n",
    "    return solution, spreads, elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['778638', '778671', '778650', '778737'], 4)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def local(stop, routes_per_stop, routes_covered, cost_per_stop):\n",
    "    return len(routes_per_stop[stop] - routes_covered)/cost_per_stop.get(stop, 1)\n",
    "\n",
    "def set_cover_greedy_gateway_selection(all_routes, routes_per_stop, cost_per_stop = {}):\n",
    "    #global all_stops, routes_per_stop\n",
    "    \"\"\"Find a family of subsets that covers the universal set\"\"\"\n",
    "    elements = set(e for s in routes_per_stop.values() for e in s)\n",
    "    # Check the subsets cover the universe\n",
    "    if elements != all_routes:\n",
    "        print(\"not all routes covered by stops\")\n",
    "        return None\n",
    "    routes_covered = set()\n",
    "    selected_gateways = []\n",
    "    total_cost = 0\n",
    "    # Greedily add the subsets with the most uncovered points\n",
    "    while routes_covered != elements:\n",
    "        selected_stop = max(routes_per_stop,\n",
    "                            #key=lambda s: local(s, routes_per_stop, routes_covered, cost_per_stop)\n",
    "                            key=lambda s: local(s, routes_per_stop, routes_covered, cost_per_stop)\n",
    "                           )\n",
    "        selected_gateways.append(selected_stop)\n",
    "        routes_covered |= routes_per_stop[selected_stop]\n",
    "        total_cost += cost_per_stop.get(selected_stop, 1)\n",
    "\n",
    "    return selected_gateways, total_cost\n",
    "\n",
    "set_cover_greedy_gateway_selection(all_routes, routes_per_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://jeremykun.com/2015/05/04/the-many-faces-of-set-cover/\n",
    "This is what theory has to say about the greedy algorithm:\n",
    "\n",
    "Theorem: If it is possible to cover U by the sets in F = \\{ S_1, \\dots, S_n \\}, then the greedy algorithm always produces a cover that at worst has size O(\\log(n)) \\textup{OPT}, where \\textup{OPT} is the size of the smallest cover. Moreover, this is asymptotically the best any algorithm can do.\n",
    "\n",
    "\n",
    "In particular, if weâ€™re guaranteed that each element x \\in U occurs in at most B of the sets S_i, then the linear programming approach will give a B-approximation, i.e. a cover whose size is at worst larger than OPT by a multiplicative factor of B. In the case that B is constant, we can beat our earlier greedy algorithm.\n",
    "\n",
    "Theorem: There is a deterministic algorithm that rounds x_{\\textup{LP}} to integer values x so that the objective value Z(x) \\leq B \\textup{OPT}_{\\textup{IP}}, where B is the maximum number of sets that any element e_j occurs in. So this gives a B-approximation of set cover.\n",
    "\n",
    "A tighter analysis for the greedy algorithm shows that the approximation ratio is exactly {\\displaystyle \\ln {n}-\\ln {\\ln {n}}+\\Theta (1)}{\\displaystyle \\ln {n}-\\ln {\\ln {n}}+\\Theta (1)}.[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_independent_cascade(graph, sensor_nodes, prob, n_iters=1000):\n",
    "    total_spead = 0\n",
    "\n",
    "    # simulate the spread process over multiple runs\n",
    "    for i in range(n_iters):\n",
    "        np.random.seed(i)\n",
    "        active = seed_nodes[:]\n",
    "        new_active = seed_nodes[:]\n",
    "        \n",
    "        # for each newly activated nodes, find its neighbors that becomes activated\n",
    "        while new_active:\n",
    "            activated_nodes = []\n",
    "            for node in new_active:\n",
    "                neighbors = graph.neighbors(node, mode='out')\n",
    "                success = np.random.uniform(0, 1, len(neighbors)) < prob\n",
    "                activated_nodes += list(np.extract(success, neighbors))\n",
    "\n",
    "            # ensure the newly activated nodes doesn't already exist\n",
    "            # in the final list of activated nodes before adding them\n",
    "            # to the final list\n",
    "            new_active = list(set(activated_nodes) - set(active))\n",
    "            active += new_active\n",
    "\n",
    "        total_spead += len(active)\n",
    "\n",
    "    return total_spead / n_iters\n",
    "\n",
    "\n",
    "# assuming we start with 1 seed node\n",
    "seed_nodes = [0]\n",
    "compute_independent_cascade(graph, seed_nodes, prob=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_route_subgraphs():\n",
    "    global G, routes_per_stop, all_routes # input\n",
    "    global route_subgraphs, stops_per_route # output\n",
    "\n",
    "    route_subgraphs = {}\n",
    "    stops_per_route = invert_dict(routes_per_stop)\n",
    "\n",
    "\n",
    "    for r in all_routes:\n",
    "        sub_nodes = [namify_stop(G.name, s) for s in stops_per_route[r]]\n",
    "        # G.remove_nodes_from([n for n in G if n not in set(nodes)])\n",
    "        sub_graph = G.subgraph(sub_nodes).copy()\n",
    "        route_subgraphs[r] = sub_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# t = trip_subgraphs['t_76016_b_17595_tn_1']\n",
    "# nx.draw(t, with_labels = True)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in all_routes:\n",
    "#         time_table\n",
    "#     break\n",
    "    \n",
    "# r\n",
    "#     #0_sub_nodes = [namify_stop(G.name, s) for s in stops_per_route[r]]\n",
    "#     # G.remove_nodes_from([n for n in G if n not in set(nodes)])\n",
    "#     #sub_graph = G.subgraph(sub_nodes).copy()\n",
    "#     #route_subgraphs[r] = sub_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# p = time_table.groupby(['route_id', 'direction_id'])['stop_id'].apply(lambda x: x.tolist()).to_dict()\n",
    "# #dep_per_route = dep_list.groupby('route_id')['deps'].apply(lambda x: x.tolist()).to_dict(into=OrderedDict)\n",
    "\n",
    "# for r in all_routes:\n",
    "#     p0 = [namify_stop(G.name, s) for s in p.get((r, 0), [])]\n",
    "#     p1 = [namify_stop(G.name, s) for s in p.get((r, 1), [])]\n",
    "#     g0 = G.subgraph(p0).copy()\n",
    "#     g1 = G.subgraph(p1).copy()\n",
    "#     #nx.draw_spectral(g0)\n",
    "#     #plt.show()\n",
    "#     n = [node for node, out_degree in g0.out_degree() if out_degree == 0]\n",
    "#     if len (g0) != 0:\n",
    "#         print(len(n), 'n', r)\n",
    "#         if len(n) == 0:\n",
    "#             nx.draw_spectral(g0)\n",
    "#             plt.show()\n",
    "#     m = [node for node, out_degree in g1.out_degree() if out_degree == 0]\n",
    "#     if len (g1) != 0:\n",
    "#         print(len(m), 'm', r)\n",
    "#         if len(m) == 0:\n",
    "#             nx.draw_spectral(g1)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in route_subgraphs.items():\n",
    "#     n = [node for node, out_degree in v.out_degree() if out_degree == 0]\n",
    "#     m = [node for node, in_degree in v.in_degree() if in_degree == 0]\n",
    "#     #n = r.out_degree()\n",
    "#     print(n)\n",
    "#     print(m)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "g = 0\n",
    "for k,v in route_subgraphs.items():\n",
    "    if nx.is_strongly_connected(v):\n",
    "        t += 1\n",
    "    g +=1\n",
    "    \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# r = route_subgraphs['18']\n",
    "# #nx.draw_kamada_kawai(r)\n",
    "# #nx.draw_spectralectral(r)\n",
    "# nx.draw_kamada_kawai(r)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [],
   "source": [
    "def generate_sensor_scenerios1(set_count, min_count, max_count):\n",
    "    global G, routes_per_stop # input\n",
    "\n",
    "    sensor_scenerios = []\n",
    "    all_stops = [get_stopid(s) for s in set(G.nodes)]\n",
    "    \n",
    "    for i in range(set_count):\n",
    "        sensor_count = random.randint(min_count, max_count)\n",
    "        all_sensors = np.random.choice(all_stops, size=sensor_count, replace=False)\n",
    "        \n",
    "        sensor_objects = {}\n",
    "\n",
    "        msg_gen_rate = np.random.randint(low = sim.msg_gen_rate_range[0], high= sim.msg_gen_rate_range[1], size=sensor_count) # 10mins to 12 hours\n",
    "        start_time = np.random.randint(low = sim.msg_gen_rate_range[0], high=sim.msg_gen_rate_range[1], size=sensor_count) # 0 to 1 hour\n",
    "        np.random.shuffle(start_time)\n",
    "\n",
    "        for i,sensor_name in enumerate(all_sensors):\n",
    "            r = routes_per_stop[get_stopid(sensor_name)]\n",
    "\n",
    "            s = OnRouteSensor(name=sensor_name, routes=r, start_time=start_time[i], msg_gen_rate=msg_gen_rate[i], msg_ttl=None, data_size=None)\n",
    "            sensor_objects[sensor_name]=s\n",
    "        sensor_scenerios.append(sensor_objects)\n",
    "        \n",
    "    return sensor_scenerios\n",
    "\n",
    "def get_sensor_score(sensor):\n",
    "    return 1/sensor.msg_gen_rate\n",
    "\n",
    "def generate_scenerios_route_scores(scenerios):\n",
    "    global stops_per_route\n",
    "    \n",
    "    scenerios_scores_per_route = []\n",
    "    for sensor_objects in scenerios:\n",
    "        score_per_route = {}\n",
    "        for r in stops_per_route.keys():\n",
    "            score = 0\n",
    "            sensors = stops_per_route[r].intersection(set(sensor_objects.keys()))\n",
    "            score = sum([get_sensor_score(sensor_objects[s]) for s in sensors])\n",
    "#             print(sensor_objects.keys())\n",
    "#             for s in sensors:\n",
    "#                 print(get_sensor_score(sensor_objects[s]))\n",
    "#                 return\n",
    "            score_per_route[r] = score\n",
    "        \n",
    "        scenerios_scores_per_route.append(score_per_route)\n",
    "    \n",
    "    return scenerios_scores_per_route\n",
    "    \n",
    "\n",
    "def local_optima2(p_gateway, routes_per_stop, routes_covered, cost_per_stop):\n",
    "    global scenerios,scenerios_route_scores\n",
    "    value = 0\n",
    "    \n",
    "    stops_served = set(e for s in (routes_per_stop[p_gateway] - routes_covered) for e in s)\n",
    "    for i,sensor_objects in enumerate(scenerios):\n",
    "        p_gateway_sensors = stops_served.intersection(set(sensor_objects.keys()))\n",
    "        for sensor in p_gateway_sensors:\n",
    "            value += get_sensor_score(sensor)\n",
    "                \n",
    "    return value/cost_per_stop.get(p_gateway, 1)\n",
    "\n",
    "def local_optima(p_gateway, routes_per_stop, routes_covered, cost_per_stop):\n",
    "    global scenerios,scenerios_route_scores\n",
    "    value = 0\n",
    "    \n",
    "    routes_served = routes_per_stop[p_gateway] - routes_covered\n",
    "    for i,_ in enumerate(scenerios):\n",
    "        for route in routes_served:\n",
    "            value += scenerios_route_scores[i][route]\n",
    "                \n",
    "    return value/cost_per_stop.get(p_gateway, 1)\n",
    "\n",
    "\n",
    "scenerios = generate_sensor_scenerios1(1, 15, 20)\n",
    "scenerios_route_scores = generate_scenerios_route_scores(scenerios)\n",
    "set_cover_greedy_gateway_selection(all_routes, routes_per_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
