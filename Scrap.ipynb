{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.2': ['778638', '778671', '778806', '778650', '778860'],\n",
       " '0.4': ['778638', '778671', '778806', '778650', '778860'],\n",
       " '0.6000000000000001': ['778638', '778671', '778650', '778737', '778784'],\n",
       " '0.8': ['778638', '778671', '778650', '778737', '778784'],\n",
       " '1.0': ['778638', '778671', '778650', '778737', '778784']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "r = None\n",
    "with open(\"mcmd2_result_5_cht.txt\", \"r\", encoding='utf-8') as file:\n",
    "    r = json.load(file)\n",
    "\n",
    "    \n",
    "    \n",
    "b = {}\n",
    "\n",
    "for k,v in r.items():\n",
    "    b[k] = v['im_gateways2']\n",
    "    \n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'0.2': ['778638', '778671', '778806', '778650', '778860'],\n",
    " '0.4': ['778638', '778671', '778806', '778650', '778860'],\n",
    " '0.6': ['778638', '778671', '778650', '778737', '778784'],\n",
    " '0.8': ['778638', '778671', '778650', '778737', '778784'],\n",
    " '1.0': ['778638', '778671', '778650', '778737', '778784']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "g = 0\n",
    "for k,v in route_subgraphs.items():\n",
    "    if nx.is_strongly_connected(v):\n",
    "        t += 1\n",
    "    g +=1\n",
    "    \n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in route_subgraphs.items():\n",
    "#     n = [node for node, out_degree in v.out_degree() if out_degree == 0]\n",
    "#     m = [node for node, in_degree in v.in_degree() if in_degree == 0]\n",
    "#     #n = r.out_degree()\n",
    "#     print(n)\n",
    "#     print(m)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# p = time_table.groupby(['route_id', 'direction_id'])['stop_id'].apply(lambda x: x.tolist()).to_dict()\n",
    "# #dep_per_route = dep_list.groupby('route_id')['deps'].apply(lambda x: x.tolist()).to_dict(into=OrderedDict)\n",
    "\n",
    "# for r in all_routes:\n",
    "#     p0 = [namify_stop(G.name, s) for s in p.get((r, 0), [])]\n",
    "#     p1 = [namify_stop(G.name, s) for s in p.get((r, 1), [])]\n",
    "#     g0 = G.subgraph(p0).copy()\n",
    "#     g1 = G.subgraph(p1).copy()\n",
    "#     #nx.draw_spectral(g0)\n",
    "#     #plt.show()\n",
    "#     n = [node for node, out_degree in g0.out_degree() if out_degree == 0]\n",
    "#     if len (g0) != 0:\n",
    "#         print(len(n), 'n', r)\n",
    "#         if len(n) == 0:\n",
    "#             nx.draw_spectral(g0)\n",
    "#             plt.show()\n",
    "#     m = [node for node, out_degree in g1.out_degree() if out_degree == 0]\n",
    "#     if len (g1) != 0:\n",
    "#         print(len(m), 'm', r)\n",
    "#         if len(m) == 0:\n",
    "#             nx.draw_spectral(g1)\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# t = trip_subgraphs['t_76016_b_17595_tn_1']\n",
    "# nx.draw(t, with_labels = True)\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for r in all_routes:\n",
    "#         time_table\n",
    "#     break\n",
    "    \n",
    "# r\n",
    "#     #0_sub_nodes = [namify_stop(G.name, s) for s in stops_per_route[r]]\n",
    "#     # G.remove_nodes_from([n for n in G if n not in set(nodes)])\n",
    "#     #sub_graph = G.subgraph(sub_nodes).copy()\n",
    "#     #route_subgraphs[r] = sub_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_scenerios1(set_count, min_count, max_count):\n",
    "    global G, routes_per_stop # input\n",
    "\n",
    "    sensor_scenerios = []\n",
    "    all_stops = [get_stopid(s) for s in set(G.nodes)]\n",
    "    \n",
    "    for i in range(set_count):\n",
    "        sensor_count = random.randint(min_count, max_count)\n",
    "        all_sensors = np.random.choice(all_stops, size=sensor_count, replace=False)\n",
    "        \n",
    "        sensor_objects = {}\n",
    "\n",
    "        msg_gen_rate = np.random.randint(low = sim.msg_gen_rate_range[0], high= sim.msg_gen_rate_range[1], size=sensor_count) # 10mins to 12 hours\n",
    "        start_time = np.random.randint(low = sim.msg_gen_rate_range[0], high=sim.msg_gen_rate_range[1], size=sensor_count) # 0 to 1 hour\n",
    "        np.random.shuffle(start_time)\n",
    "\n",
    "        for i,sensor_name in enumerate(all_sensors):\n",
    "            r = routes_per_stop[get_stopid(sensor_name)]\n",
    "\n",
    "            s = OnRouteSensor(name=sensor_name, routes=r, start_time=start_time[i], msg_gen_rate=msg_gen_rate[i], msg_ttl=None, data_size=None)\n",
    "            sensor_objects[sensor_name]=s\n",
    "        sensor_scenerios.append(sensor_objects)\n",
    "        \n",
    "    return sensor_scenerios\n",
    "\n",
    "def get_sensor_score(sensor):\n",
    "    return 1/sensor.msg_gen_rate\n",
    "\n",
    "def generate_scenerios_route_scores(scenerios):\n",
    "    global stops_per_route\n",
    "    \n",
    "    scenerios_scores_per_route = []\n",
    "    for sensor_objects in scenerios:\n",
    "        score_per_route = {}\n",
    "        for r in stops_per_route.keys():\n",
    "            score = 0\n",
    "            sensors = stops_per_route[r].intersection(set(sensor_objects.keys()))\n",
    "            score = sum([get_sensor_score(sensor_objects[s]) for s in sensors])\n",
    "#             print(sensor_objects.keys())\n",
    "#             for s in sensors:\n",
    "#                 print(get_sensor_score(sensor_objects[s]))\n",
    "#                 return\n",
    "            score_per_route[r] = score\n",
    "        \n",
    "        scenerios_scores_per_route.append(score_per_route)\n",
    "    \n",
    "    return scenerios_scores_per_route\n",
    "    \n",
    "\n",
    "def local_optima2(p_gateway, routes_per_stop, routes_covered, cost_per_stop):\n",
    "    global scenerios,scenerios_route_scores\n",
    "    value = 0\n",
    "    \n",
    "    stops_served = set(e for s in (routes_per_stop[p_gateway] - routes_covered) for e in s)\n",
    "    for i,sensor_objects in enumerate(scenerios):\n",
    "        p_gateway_sensors = stops_served.intersection(set(sensor_objects.keys()))\n",
    "        for sensor in p_gateway_sensors:\n",
    "            value += get_sensor_score(sensor)\n",
    "                \n",
    "    return value/cost_per_stop.get(p_gateway, 1)\n",
    "\n",
    "def local_optima(p_gateway, routes_per_stop, routes_covered, cost_per_stop):\n",
    "    global scenerios,scenerios_route_scores\n",
    "    value = 0\n",
    "    \n",
    "    routes_served = routes_per_stop[p_gateway] - routes_covered\n",
    "    for i,_ in enumerate(scenerios):\n",
    "        for route in routes_served:\n",
    "            value += scenerios_route_scores[i][route]\n",
    "                \n",
    "    return value/cost_per_stop.get(p_gateway, 1)\n",
    "\n",
    "\n",
    "scenerios = generate_sensor_scenerios1(1, 15, 20)\n",
    "scenerios_route_scores = generate_scenerios_route_scores(scenerios)\n",
    "set_cover_greedy_gateway_selection(all_routes, routes_per_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapdict import heapdict\n",
    "import copy\n",
    "import time\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, node):\n",
    "        self.node = node\n",
    "        self.mg1 = 0\n",
    "        self.prev_best = None\n",
    "        self.mg2 = 0\n",
    "        self.flag = None\n",
    "        self.list_index = 0\n",
    "\n",
    "\n",
    "def celfpp_v2(G, budget, n_scenerios, min_sensor_count=10, max_sensor_count=30):\n",
    "    start_time = time.time()\n",
    "    scenerios = generate_sensor_scenerios(n_scenerios, min_sensor_count, max_sensor_count)\n",
    "\n",
    "    graph = copy.deepcopy(G)\n",
    "    S = set()\n",
    "    # Note that heapdict is min heap and hence add negative priorities for\n",
    "    # it to work.\n",
    "    Q = heapdict()\n",
    "    last_seed = None\n",
    "    cur_best = None\n",
    "    node_data_list = []\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        node_data = Node(node)\n",
    "        delay = compute_delay(graph, [node], scenerios)\n",
    "        node_data.mg1 = sim.upper_bound_delay - delay\n",
    "        # node_data.mg1 = diffuse.diffuse_mc([node])\n",
    "        node_data.prev_best = cur_best\n",
    "        node_data.mg2 = compute_delay(graph, [node, cur_best.node], scenerios) if cur_best else node_data.mg1\n",
    "        node_data.flag = 0\n",
    "        cur_best = cur_best if cur_best and cur_best.mg1 > node_data.mg1 else node_data\n",
    "        graph.nodes[node]['node_data'] = node_data\n",
    "        node_data_list.append(node_data)\n",
    "        node_data.list_index = len(node_data_list) - 1\n",
    "        Q[node_data.list_index] = - node_data.mg1\n",
    "\n",
    "    # record the number of times the spread is computed\n",
    "    lookups = [graph.number_of_nodes()]\n",
    "    elapsed = [round(time.time() - start_time, 3)]\n",
    "\n",
    "    while len(S) < budget:\n",
    "        node_idx, _ = Q.peekitem()\n",
    "        node_data = node_data_list[node_idx]\n",
    "        if node_data.flag == len(S):\n",
    "            S.add(node_data.node)\n",
    "            del Q[node_idx]\n",
    "            last_seed = node_data\n",
    "            continue\n",
    "        elif node_data.prev_best == last_seed:\n",
    "            node_data.mg1 = node_data.mg2\n",
    "        else:\n",
    "            # before = diffuse.diffuse_mc(S)\n",
    "            delay = compute_delay(graph, S, scenerios)\n",
    "            before = sim.upper_bound_delay - delay\n",
    "            S.add(node_data.node)\n",
    "            delay = compute_delay(graph, S, scenerios)\n",
    "            after = sim.upper_bound_delay - delay\n",
    "            S.remove(node_data.node)\n",
    "            node_data.mg1 = after - before\n",
    "            node_data.prev_best = cur_best\n",
    "            S.add(cur_best.node)\n",
    "            delay = compute_delay(graph, S, scenerios)\n",
    "            before = sim.upper_bound_delay - delay\n",
    "            S.add(node_data.node)\n",
    "            delay = compute_delay(graph, S, scenerios)\n",
    "            after = sim.upper_bound_delay - delay\n",
    "            S.remove(cur_best.node)\n",
    "            if node_data.node != cur_best.node: S.remove(node_data.node)\n",
    "            node_data.mg2 = after - before\n",
    "\n",
    "        if cur_best and cur_best.mg1 < node_data.mg1:\n",
    "            cur_best = node_data\n",
    "\n",
    "        node_data.flag = len(S)\n",
    "        Q[node_idx] = - node_data.mg1\n",
    "\n",
    "    return S, elapsed, lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import time\n",
    "\n",
    "def celfpp_im(graph, budget, n_scenerios, min_sensor_count=10, max_sensor_count=30):\n",
    "    \"\"\"\n",
    "    A CELF++ algorithms for finding proper seed set with a predefined size to gain a \n",
    "        maximum influence of a network graph.\n",
    "       seeds: the set containing all seeds.\n",
    "       graph: the graph object.\n",
    "       size_seed: the size of chosen seed set.\n",
    "       mode: str, \"IC\" or \"LT\"\n",
    "    \"\"\"\n",
    "    start_time = time.time() # in seconds\n",
    "    scenerios = generate_sensor_scenerios(n_scenerios, min_sensor_count, max_sensor_count)\n",
    "\n",
    "    best_inverse_delay = float('-inf')\n",
    "    gateways = []\n",
    "\n",
    "    pq = [[1, -1, -1, -1]] # node is an array, the first is the marginal value, the second is the node index and\n",
    "                        # last is the size of temporary seed set.\n",
    "    # initialization\n",
    "    for node in set(graph.nodes):\n",
    "        delay = compute_delay(graph, [node], scenerios)\n",
    "        delay_gain = sim.upper_bound_delay - delay\n",
    "        heapq.heappush(pq, [-delay_gain, node, 0, delay])\n",
    "\n",
    "    node = heapq.heappop(pq)\n",
    "    gateways.append(node[1])\n",
    "    best_inverse_delay = -node[0]\n",
    "    #best_delay = delay_gain\n",
    "    #delay_gains = [delay_gain]\n",
    "    delays = [node[3]]\n",
    "\n",
    "\n",
    "    # record the number of times the spread is computed\n",
    "    lookups = [graph.number_of_nodes()]\n",
    "    elapsed = [round(time.time() - start_time, 3)]\n",
    "\n",
    "    while len(gateways) < budget:\n",
    "        node_lookup = 0\n",
    "        while True:\n",
    "            node_lookup += 1\n",
    "\n",
    "            #tmp_set = set(gateways)\n",
    "            node = heapq.heappop(pq)\n",
    "            if node[2] == len(gateways):\n",
    "                gateways.append(node[1])\n",
    "                delays.append(node[3])\n",
    "                lookups.append(node_lookup)\n",
    "                elapse = round(time.time() - start_time, 3)\n",
    "                elapsed.append(elapse)\n",
    "                break\n",
    "\n",
    "            #tmp_set.add(node[1])\n",
    "            #diff_spread = cal_spread(graph, tmp_set)\n",
    "            # diff_spread = compute_delay(graph, tmp_set, scenerios)\n",
    "            delay = compute_delay(graph, gateways + [node[1]], scenerios)\n",
    "            diff_spread = sim.upper_bound_delay - delay\n",
    "\n",
    "            top = pq[0]\n",
    "            if diff_spread - best_inverse_delay >= -top[0]:\n",
    "                gateways.append(node[1])\n",
    "                delays.append(delay)\n",
    "                best_inverse_delay = diff_spread\n",
    "                lookups.append(node_lookup)\n",
    "                elapse = round(time.time() - start_time, 3)\n",
    "                elapsed.append(elapse)\n",
    "                #delay_gains.append(delay_gain)\n",
    "                if len(gateways) == budget:\n",
    "                    break\n",
    "            else:\n",
    "                node[0] = -(diff_spread - best_inverse_delay)\n",
    "                node[2] = len(gateways)\n",
    "                node[3] = delay\n",
    "                heapq.heappush(pq, node)\n",
    "\n",
    "    return gateways, delays, elapsed, lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sensor_scenerios(set_count, min_sensors, max_sensors):\n",
    "    import random\n",
    "    global G, routes_per_stop, all_routes  # input\n",
    "    #global sensor_scenerios #output\n",
    "    \n",
    "    #TODO:: seed works only if called from within where it is set\n",
    "    #sim.seed = 0\n",
    "    np.random.seed(sim.seed)\n",
    "    random.seed(sim.seed)\n",
    "\n",
    "    sensor_scenerios = []\n",
    "    all_stops = [s for s in set(G.nodes)]\n",
    "\n",
    "    for _ in range(set_count):\n",
    "        sensor_count = random.randint(min_sensors, max_sensors)\n",
    "        scenerio = Scenerio(graph=G, \n",
    "                            all_stops= all_stops,\n",
    "                            all_routes= all_routes,\n",
    "                            routes_per_stop=routes_per_stop,\n",
    "                            sensor_count=sensor_count\n",
    "                            )\n",
    "        \n",
    "        sensor_scenerios.append(scenerio)\n",
    "        \n",
    "    return sensor_scenerios\n",
    "        \n",
    "#generate_sensor_scenerios(2, 20, 50)\n",
    "\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "def celf_im(graph, budget, n_scenerios, min_sensor_count=10, max_sensor_count=30):\n",
    "    \"\"\"\n",
    "    Find k nodes with the largest spread (determined by IC) from a igraph graph\n",
    "    using the Cost Effective Lazy Forward Algorithm, a.k.a Lazy Greedy Algorithm.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    scenerios = generate_sensor_scenerios(n_scenerios, min_sensor_count, max_sensor_count)\n",
    "\n",
    "    # find the first node with greedy algorithm:\n",
    "    # TODO:: python's heap is a min-heap, thus\n",
    "    # TODO:: we negate the spread to get the node\n",
    "    # TODO:: with the maximum spread when popping from the heap\n",
    "    print(\"started\")\n",
    "    gains = []\n",
    "    for node in set(graph.nodes):\n",
    "        delay = compute_delay(graph, [node], scenerios)\n",
    "        delay_gain = sim.upper_bound_delay - delay\n",
    "        heapq.heappush(gains, (-delay_gain, node, delay))\n",
    "\n",
    "    # we pop the heap to get the node with the best spread,\n",
    "    # TODO:: when storing the spread to negate it again to store the actual spread\n",
    "    delay_gain, node, delay = heapq.heappop(gains)\n",
    "    delay_gain = -delay_gain\n",
    "    gateways = [node]\n",
    "    delay_gains = [delay_gain]\n",
    "    delays = [delay]\n",
    "\n",
    "    # record the number of times the spread is computed\n",
    "    lookups = [graph.number_of_nodes()]\n",
    "    elapsed = [round(time.time() - start_time, 3)]\n",
    "\n",
    "    for _ in range(budget - 1):\n",
    "        node_lookup = 0\n",
    "        matched = False\n",
    "\n",
    "        while not matched:\n",
    "            node_lookup += 1\n",
    "\n",
    "            # TODO:: here we need to compute the marginal gain of adding the current node\n",
    "            # to the solution, instead of just the gain, i.e. we need to subtract\n",
    "            # the spread without adding the current node\n",
    "            _, current_node, _ = heapq.heappop(gains)\n",
    "            delay = compute_delay(graph, gateways + [current_node], scenerios)\n",
    "            new_delay_gain = (sim.upper_bound_delay - delay)- delay_gain\n",
    "\n",
    "            # check if the previous top node stayed on the top after pushing\n",
    "            # the marginal gain to the heap\n",
    "            heapq.heappush(gains, (-new_delay_gain, current_node, delay))\n",
    "            matched = (gains[0][1] == current_node)\n",
    "\n",
    "        # spread stores the cumulative spread\n",
    "        new_delay_gain, node, delay = heapq.heappop(gains)\n",
    "        delay_gain -= new_delay_gain\n",
    "        gateways.append(node)\n",
    "        delay_gains.append(delay_gain)\n",
    "        delays.append(delay)\n",
    "        lookups.append(node_lookup)\n",
    "\n",
    "        elapse = round(time.time() - start_time, 3)\n",
    "        elapsed.append(elapse)\n",
    "\n",
    "    return gateways, delays, elapsed, lookups # delay_gains"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
